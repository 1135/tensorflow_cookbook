{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning with TensorFlow\n",
    "\n",
    "We will create a q-Learning algorithm to learn how to play 'Connect 3'. Similar to the famous Connect 4, but with far fewer possible states.\n",
    "\n",
    "Here is the size of the 'Connect 3' board (4x4) board:\n",
    "\n",
    "| Col 1 | Col 2 | Col 3 | Col 4 |\n",
    "|-|-|-|-|\n",
    "|-|-|-|-|\n",
    "|-|-|-|-|\n",
    "|-|-|-|-|\n",
    "|-|-|-|-|\n",
    "\n",
    "We will denote a board with x's and o's.  A potential board could look like:\n",
    "\n",
    "| Col 1 | Col 2 | Col 3 | Col 4 |\n",
    "|-|-|-|-|\n",
    "| |x| | |\n",
    "| |o| | |\n",
    "|o|x| | |\n",
    "|x|x|o|o|\n",
    "\n",
    "We will store the board states as a dictionary, with the board being stored as key and the row of the reward matrix being the key.\n",
    "\n",
    "For example, if we observe the above state of the board, and we have not observed it before, we add another key to the dictionary, 'eeeeexeeeoeeoxeexxoe' where 'e' stands for empty, and the key is read in reading order (top left to bottom right).  All keys will be 16 letters long.\n",
    "\n",
    "```python\n",
    "board_dict['eeeeexeeeoeeoxeexxoo'] = reward_matrix.shape[0] + 1\n",
    "new_row = [0, 0, 0, 0]\n",
    "reward_matrix = np.vstack([reward_matrix, new_row])\n",
    "```\n",
    "\n",
    "Note that the `new_row` vector only has four possible reward states because there are only four possible actions. These actions are to drop your piece in column 1, 2, 3, or 4.\n",
    "\n",
    "### Q Learning Methodology\n",
    "------------------\n",
    "\n",
    "Q-learning relies on a table lookup.  That table is called a Q-table and the usage of it is to lookup the current state, and take the action that has the highest Q-value.\n",
    "\n",
    "In other words, Q-learning methodology is essentially looking up the current state in the Q-table, and choosing the action that has the highest Q value to progress onto the next state.\n",
    "\n",
    "We also update the Q table according to the following equation:\n",
    "\n",
    "$$Q(s_{i},a_{j}) = r + \\gamma \\left( \\max_{a \\in A_{i+1}} Q(s_{i+1}, a) \\right)$$\n",
    "\n",
    "Where $A_{i+1}$ is set of all possible actions in state $s_{i+1}$.  In other words, our updated value in position $(s_{i}, a_{j})$ in the Q-table is a reward, $r$, plus a discounted Q-value for the next step.  The discounted amount is given by $\\gamma$, which is a positive fraction less than one (a common value is $\\gamma=0.99$).\n",
    "\n",
    "### This Simulation\n",
    "-------------------------\n",
    "\n",
    "For this specific game simulation, we should define some specifics.  The reward, $r$, for a move will be zero, unless we achieve four in a row, then the reward will be 1.\n",
    "\n",
    "While it may be tempting to make the reward proportional to how many we can get in a row (e.g. make the reward 0.5 for getting two in a row), doing this may force the algorithm to adopt a non-optimal strategy.  For example, maybe it is better to try to connect two pieces separated by 1 space than to build from 1 to 2 to 3 in a row.\n",
    "\n",
    "When the algorithm does get a win, eventually the reward will propagate backwards to decisions that have lead it to win.\n",
    "\n",
    "We set the descounted gain as $\\gamma = 0.99$ and the learning rate (which is how much we change the current Q values, to $\\eta = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = np.array([[1,2],[3,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43046721"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(3, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-20 15:06:55,856] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .85\n",
    "y = .99\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    jList.append(j)\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.4765\n",
      "Final Q-Table Values: [[  1.27149803e-02   6.67873088e-01   7.88563722e-03   3.59903163e-03]\n",
      " [  1.63838130e-03   1.64431327e-03   2.46295778e-04   4.46821192e-01]\n",
      " [  2.02305140e-04   3.76659172e-03   8.21697654e-03   2.50961821e-01]\n",
      " [  1.30828329e-04   9.08831267e-04   8.90749479e-04   1.55891436e-01]\n",
      " [  4.18951127e-01   2.37370694e-02   2.47074568e-03   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  2.06599254e-05   9.91590650e-06   1.69375005e-04   8.74995582e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  5.18448283e-03   6.07880612e-03   0.00000000e+00   7.95344009e-01]\n",
      " [  0.00000000e+00   9.02184277e-01   2.36259318e-03   0.00000000e+00]\n",
      " [  9.67735898e-01   0.00000000e+00   0.00000000e+00   7.64250441e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   7.60899237e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   9.97982000e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Score over time: {}\".format(sum(rList)/num_episodes))\n",
    "print(\"Final Q-Table Values: {}\".format(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f3a32064748>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 False {'prob': 0.3333333333333333}\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x7f3a32064748>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_s, r, is_end, inf = env.step(action=3)\n",
    "print(next_s, r, is_end, inf)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'FrozenLakeEnv' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ebc04aebb515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'FrozenLakeEnv' object is not iterable"
     ]
    }
   ],
   "source": [
    "[x for x in env]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
